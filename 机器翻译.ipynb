{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import jieba\n",
    "jieba.setLogLevel(jieba.logging.INFO)  # 屏蔽jieba分词时出现的提示信息\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30  # 句子的最大长度\n",
    "# 加载数据集\n",
    "path = r'C:\\Users\\86187\\Desktop\\seg2seg翻译\\translate'\n",
    "fg = open(path + '\\\\' + \"en_zh_data.txt\", encoding='utf-8')\n",
    "lines = list(fg)\n",
    "fg.close()\n",
    "pairs = []\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    pair = line.split('--->')  # 英中文句子以字符串'--->'隔开\n",
    "    if len(pair) != 2:\n",
    "        continue\n",
    "    en_sen = pair[0]  # 英文句子\n",
    "    zh_sen = pair[1]  # 中文句子\n",
    "    pairs.append([en_sen, zh_sen])\n",
    "pairs = pairs[:50]  # 为了节省调试时间，只用50对英中文句子来训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(pairs):\n",
    "    temp = []\n",
    "    for pair in pairs:\n",
    "        split_eng = pair[0].split(' ')  # 切分英文单词\n",
    "        split_chi = [word for word in jieba.cut(pair[1]) if word != ' ']  # 中文分词\n",
    "        if len(split_eng) < MAX_LENGTH and len(split_chi) < MAX_LENGTH:\n",
    "            temp.append(pair)  # 保留长度小于MAX_LENGTH的句子对\n",
    "    pairs = temp\n",
    "    eng_lang = Word_Dict('eng')  # 初始化中文字典\n",
    "    chi_lang = Word_Dict('chi')  # 初始化英文字典\n",
    "    for pair in pairs:  # 对每个句子对构造字典\n",
    "        eng_lang.addOneSentence(pair[0])  # 建立英文单词索引字典\n",
    "        chi_lang.addOneSentence(pair[1])  # 建立中文词索引字典\n",
    "    return eng_lang, chi_lang, pairs  # 返回构造好的英文字典和中文字典，以及符合长度的句子对\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语言文本词汇的字典类Word_Dict\n",
    "class Word_Dict:  # 构建词典，为每个词确定一个唯一的索引号\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # 指处理中文还是英文\n",
    "        self.word2index = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<UNK>\", 2: \"<SOS>\", 3: \"<EOS>\"}\n",
    "    def addOneSentence(self, sentence):\n",
    "        if self.name == 'eng':\n",
    "            for word in sentence.split(' '):  # 英文的话 用split(' ')分词\n",
    "                self.addOneWord(word)  #\n",
    "        elif self.name == 'chi':  # 中文的话 用jieba分词\n",
    "            split_chi = [char for char in jieba.cut(sentence) if char != ' ']\n",
    "            for word in split_chi:\n",
    "                self.addOneWord(word)\n",
    "    def addOneWord(self, word):  # 将词加入到字典中\n",
    "        if word not in self.word2index:\n",
    "            index = len(self.index2word)\n",
    "            self.word2index[word] = index\n",
    "            self.index2word[index] = word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor(lang, sentence,flag):\n",
    "    indexes = []\n",
    "    if flag=='encoder_in': #编码器的输入（英文句子）\n",
    "        words = [word for word in sentence.split(' ') if word.strip() != '']  # 分词\n",
    "        words = words[0:MAX_LENGTH]\n",
    "        words = words + ['<PAD>']*(MAX_LENGTH-len(words)) #等长化\n",
    "        indexes = [lang.word2index.get(word, 1) for word in words] # 1为未知单词'<UNK>'的索引\n",
    "    elif flag == 'decoder_in':  # 解码器的输入（中文句子）\n",
    "        words = [word for word in jieba.cut(sentence) if word.strip() != '']  # 分词\n",
    "        words = ['<SOS>'] + words\n",
    "        words = words[0:MAX_LENGTH]\n",
    "        words = words + ['<PAD>'] * (MAX_LENGTH - len(words))  # 等长化\n",
    "        indexes = [lang.word2index.get(word, 1) for word in words]\n",
    "    elif flag == 'decoder_out':  # 解码器的期望输出（中文句子）\n",
    "        words = [word for word in jieba.cut(sentence) if word.strip() != '']  # 分词\n",
    "        words = words[0:MAX_LENGTH-1] #保证下面添加的结束符'<EOS>'不被截出\n",
    "        words = words + ['<EOS>']\n",
    "        words = words + ['<PAD>'] * (MAX_LENGTH - len(words))  # 等长化\n",
    "        indexes = [lang.word2index.get(word, 1) for word in words]\n",
    "    else:\n",
    "        pass\n",
    "    return torch.LongTensor(indexes).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        en_sentence = pair[0] #英文句子\n",
    "        zh_sentence = pair[1] #中文句子\n",
    "        # 传入英文字典和英文句子，返回输入编码器的张量\n",
    "        en_input = sentence2tensor(eng_lang, en_sentence, flag='encoder_in')\n",
    "        # 传入中文字典和中文句子，返回输入解码器的张量\n",
    "        de_input = sentence2tensor(chi_lang, zh_sentence, flag='decoder_in')\n",
    "        # 传入中文字典和中文句子，返回输出解码器的张量（期望输出，即输入的标记）\n",
    "        de_output = sentence2tensor(chi_lang, zh_sentence, flag='decoder_out')\n",
    "        return en_input,de_input,de_output\n",
    "\n",
    "\n",
    "# eng_lang=构造好的英文字典   chi_lang=构造好的中文字典   pairs=符合长度的句子对\n",
    "eng_lang, chi_lang, pairs = getData(pairs)\n",
    "\n",
    "mydataset = MyDataSet(pairs)\n",
    "loader = DataLoader(mydataset, batch_size=9, shuffle=True)\n",
    "#torch.Size([9, 30])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers位置编码\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, d_model,  max_len=MAX_LENGTH): #\n",
    "        super(PosEncoding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model #d_model需为偶数\n",
    "    def forward(self, x):  # torch.Size([9, 30])\n",
    "        p = torch.arange(0, self.max_len).float().unsqueeze(1)  #\n",
    "        p_2i = torch.arange(0, self.d_model, 2)\n",
    "        p_2i = 1./np.power(10000.0, (p_2i.float() / self.d_model))\n",
    "        pos_code = torch.zeros(self.max_len, self.d_model)  # torch.Size([30, 256])\n",
    "        pos_code[:, 0::2] = torch.sin(p * p_2i)  # <----torch.Size([30, 128])\n",
    "        pos_code[:, 1::2] = torch.cos(p * p_2i)  # <----torch.Size([30, 128])\n",
    "        pos_code = pos_code.to(device)  #  torch.Size([30, 256])  每个位置有一个位置向量了\n",
    "        o = pos_code[:x.size(1)]  #x.size(1)为句子的长度\n",
    "        o = o.unsqueeze(0)  #增加第一个维度，大小为1，表示有一个句子，这是o包含了一个句子中每个位置的位置编码（向量）  torch.Size([1, 30, 256])\n",
    "        o = o.repeat(x.size(0), 1, 1)  # 每个句子中，相同位置的元素，它们的位置向量是相同的，因此复制即可\n",
    "        o = o.permute([1, 0, 2])  #修改形状，改为(seq_len, batch_size, d_model) torch.Size([30, 9, 256])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers位置编码，用于初始化嵌入层\n",
    "def PosEncoding_for_Embedding(d_model,  max_len=MAX_LENGTH): #\n",
    "    p = torch.arange(0, max_len).float().unsqueeze(1)  #\n",
    "    p_2i = torch.arange(0, d_model, 2)\n",
    "    p_2i = 1./np.power(10000.0, (p_2i.float() / d_model))\n",
    "    pos_code = torch.zeros(max_len, d_model)  # torch.Size([30, 256])\n",
    "    pos_code[:, 0::2] = torch.sin(p * p_2i)  # <----torch.Size([30, 128])\n",
    "    pos_code[:, 1::2] = torch.cos(p * p_2i)  # <----torch.Size([30, 128])\n",
    "    pos_code = pos_code.to(device)  #  torch.Size([30, 256])  每个位置有一个位置向量了\n",
    "    return pos_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_code(x): #torch.Size([9, 30]\n",
    "    one_sen_poses = [pos for pos in range(x.size(1))]\n",
    "    all_sen_poses = torch.LongTensor(one_sen_poses).unsqueeze(0).to(device)\n",
    "    all_sen_poses = all_sen_poses.repeat(x.size(0),1) #torch.Size([9, 30])\n",
    "    return all_sen_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    #                    256       4        2       512       310            334\n",
    "    def __init__(self, d_model, nhead, layer_num, dim_ff, src_vocab_size, tgt_vocab_size):\n",
    "        super(MyTransformer, self).__init__()\n",
    "        #利用调用nn.Transformer()来实例化类的对象，构建Transformer模型\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=layer_num,\n",
    "                                          num_decoder_layers=layer_num,\n",
    "                                          dim_feedforward=dim_ff)\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model) #定义面向英文单词的嵌入层\n",
    "\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model) #定义面向中文词的嵌入层\n",
    "        self.pos_encoding = PosEncoding(d_model, max_len=MAX_LENGTH)\n",
    "        #在本例中源句子和目标句子的最大长度设置为一样长，故可共享编码函数PosEncoding_for_Embedding\n",
    "        #self.pos_embedding = nn.Embedding.from_pretrained(\\\n",
    "        #    PosEncoding_for_Embedding(d_model, MAX_LENGTH), freeze=True)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, en_input, de_input):  # torch.Size([9, 30]) torch.Size([9, 30])\n",
    "        cur_len = de_input.shape[1] #获取目标句子的固定长度\n",
    "        #产生一个三角掩码矩阵\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(cur_len).to(device)\n",
    "        src_key_padding_mask = en_input.data.eq(0).to(device)  #产生编码器输入的布尔掩码矩阵\n",
    "        tgt_key_padding_mask = de_input.data.eq(0).to(device)  #产生解码器输入的布尔掩码矩阵\n",
    "\n",
    "        #对编码器输入进行嵌入表示\n",
    "        src_emb=self.src_embedding(en_input).permute([1,0,2])\n",
    "        #对编码器输入进行位置编码\n",
    "        src_pos_code=self.pos_encoding(en_input)\n",
    "        #嵌入向量+位置向量=编码器的输入向量\n",
    "        en_inputs=src_emb+src_pos_code\n",
    "\n",
    "        tgt_emb = self.tgt_embedding(de_input).permute([1, 0, 2]) #对解码器输入进行嵌入表示 #torch.Size([30, 9, 256])\n",
    "        tgt_pos_code = self.pos_encoding(de_input) #对解码器输入进行位置编码\n",
    "        #tgt_pos_emb = self.pos_embedding(pos_code(de_input)).permute([1, 0, 2])\n",
    "\n",
    "\n",
    "\n",
    "        de_inputs = tgt_emb + tgt_pos_code #嵌入向量加上位置向量，构成解码器的输入向量 torch.Size([30, 9, 256])\n",
    "        #de_inputs = tgt_emb + tgt_pos_emb\n",
    "\n",
    "\n",
    "        # 送入Transformer，dec_outputs和de_input的形状相同\n",
    "        dec_outputs = self.transformer(src=en_inputs, tgt=de_inputs,\n",
    "                                       tgt_mask = tgt_mask,\n",
    "                                       src_key_padding_mask = src_key_padding_mask,\n",
    "                                       tgt_key_padding_mask = tgt_key_padding_mask)\n",
    "\n",
    "        #self.transformer(src=en_inputs, tgt=de_inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tmp = self.fc(dec_outputs.transpose(0, 1))  #对Transformer的输出进行调整，使输出尺寸为目标语言的词汇数torch.Size([9, 30, 305])\n",
    "        de_pre_y = tmp.view(-1, tmp.size(-1))  # torch.Size([270, 305])\n",
    "        return de_pre_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 参数\n",
    "d_model = 256  # 嵌入向量的长度\n",
    "nhead = 4      # 多头注意力的头个数\n",
    "layer_num = 2   # 编码器和解码器的层数\n",
    "dim_ff = 512  # FeedForward 的维度  隐含层神经元个数??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(eng_lang.word2index)  # 输入字典的单词个数 英文字典  构造embdding层\n",
    "tgt_vocab_size = len(chi_lang.word2index)  # 输出字典的单词个数  中文字典    构造embdding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                      Embedding维度     注意力的头个数     编码器和解码器的层数\n",
    "transformer_model = MyTransformer(d_model=d_model, nhead=nhead, layer_num=layer_num,\n",
    "                                  # FeedForward 的维度          源词汇数                      目标词汇数\n",
    "                                  dim_ff=dim_ff, src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(transformer_model.parameters(), lr=1e-3, momentum=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86187\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 0.787946\n",
      "Epoch: 0002 loss = 0.780089\n",
      "Epoch: 0003 loss = 0.755730\n",
      "Epoch: 0004 loss = 0.739771\n",
      "Epoch: 0005 loss = 0.731053\n",
      "Epoch: 0006 loss = 0.721300\n",
      "Epoch: 0007 loss = 0.723370\n",
      "Epoch: 0008 loss = 0.711970\n",
      "Epoch: 0009 loss = 0.704137\n",
      "Epoch: 0010 loss = 0.704064\n",
      "Epoch: 0011 loss = 0.706693\n",
      "Epoch: 0012 loss = 0.703201\n",
      "Epoch: 0013 loss = 0.703725\n",
      "Epoch: 0014 loss = 0.687165\n",
      "Epoch: 0015 loss = 0.686993\n",
      "Epoch: 0016 loss = 0.680739\n",
      "Epoch: 0017 loss = 0.686960\n",
      "Epoch: 0018 loss = 0.687289\n",
      "Epoch: 0019 loss = 0.666697\n",
      "Epoch: 0020 loss = 0.667400\n",
      "Epoch: 0021 loss = 0.656787\n",
      "Epoch: 0022 loss = 0.655430\n",
      "Epoch: 0023 loss = 0.642056\n",
      "Epoch: 0024 loss = 0.630659\n",
      "Epoch: 0025 loss = 0.617860\n",
      "Epoch: 0026 loss = 0.605604\n",
      "Epoch: 0027 loss = 0.584035\n",
      "Epoch: 0028 loss = 0.574656\n",
      "Epoch: 0029 loss = 0.563554\n",
      "Epoch: 0030 loss = 0.551092\n",
      "Epoch: 0031 loss = 0.529906\n",
      "Epoch: 0032 loss = 0.516321\n",
      "Epoch: 0033 loss = 0.508263\n",
      "Epoch: 0034 loss = 0.497326\n",
      "Epoch: 0035 loss = 0.483093\n",
      "Epoch: 0036 loss = 0.463270\n",
      "Epoch: 0037 loss = 0.457315\n",
      "Epoch: 0038 loss = 0.438556\n",
      "Epoch: 0039 loss = 0.425312\n",
      "Epoch: 0040 loss = 0.412874\n",
      "Epoch: 0041 loss = 0.401456\n",
      "Epoch: 0042 loss = 0.385890\n",
      "Epoch: 0043 loss = 0.368124\n",
      "Epoch: 0044 loss = 0.350275\n",
      "Epoch: 0045 loss = 0.335308\n",
      "Epoch: 0046 loss = 0.314817\n",
      "Epoch: 0047 loss = 0.302952\n",
      "Epoch: 0048 loss = 0.287350\n",
      "Epoch: 0049 loss = 0.275036\n",
      "Epoch: 0050 loss = 0.262250\n",
      "Epoch: 0051 loss = 0.246060\n",
      "Epoch: 0052 loss = 0.240759\n",
      "Epoch: 0053 loss = 0.223665\n",
      "Epoch: 0054 loss = 0.213279\n",
      "Epoch: 0055 loss = 0.196079\n",
      "Epoch: 0056 loss = 0.188262\n",
      "Epoch: 0057 loss = 0.174646\n",
      "Epoch: 0058 loss = 0.160001\n",
      "Epoch: 0059 loss = 0.153784\n",
      "Epoch: 0060 loss = 0.139532\n",
      "Epoch: 0061 loss = 0.130718\n",
      "Epoch: 0062 loss = 0.119129\n",
      "Epoch: 0063 loss = 0.109757\n",
      "Epoch: 0064 loss = 0.109809\n",
      "Epoch: 0065 loss = 0.100297\n",
      "Epoch: 0066 loss = 0.094667\n",
      "Epoch: 0067 loss = 0.084673\n",
      "Epoch: 0068 loss = 0.085882\n",
      "Epoch: 0069 loss = 0.077346\n",
      "Epoch: 0070 loss = 0.071402\n",
      "Epoch: 0071 loss = 0.065623\n",
      "Epoch: 0072 loss = 0.060872\n",
      "Epoch: 0073 loss = 0.060618\n",
      "Epoch: 0074 loss = 0.056125\n",
      "Epoch: 0075 loss = 0.048502\n",
      "Epoch: 0076 loss = 0.046293\n",
      "Epoch: 0077 loss = 0.043015\n",
      "Epoch: 0078 loss = 0.042850\n",
      "Epoch: 0079 loss = 0.040299\n",
      "Epoch: 0080 loss = 0.036839\n",
      "Epoch: 0081 loss = 0.034007\n",
      "Epoch: 0082 loss = 0.032815\n",
      "Epoch: 0083 loss = 0.030056\n",
      "Epoch: 0084 loss = 0.028364\n",
      "Epoch: 0085 loss = 0.026709\n",
      "Epoch: 0086 loss = 0.027344\n",
      "Epoch: 0087 loss = 0.026130\n",
      "Epoch: 0088 loss = 0.022214\n",
      "Epoch: 0089 loss = 0.021726\n",
      "Epoch: 0090 loss = 0.019627\n",
      "Epoch: 0091 loss = 0.018841\n",
      "Epoch: 0092 loss = 0.018910\n",
      "Epoch: 0093 loss = 0.017025\n",
      "Epoch: 0094 loss = 0.017568\n",
      "Epoch: 0095 loss = 0.015734\n",
      "Epoch: 0096 loss = 0.015500\n",
      "Epoch: 0097 loss = 0.013486\n",
      "Epoch: 0098 loss = 0.013187\n",
      "Epoch: 0099 loss = 0.014281\n",
      "Epoch: 0100 loss = 0.012530\n",
      "Epoch: 0101 loss = 0.012601\n",
      "Epoch: 0102 loss = 0.012817\n",
      "Epoch: 0103 loss = 0.010890\n",
      "Epoch: 0104 loss = 0.009895\n",
      "Epoch: 0105 loss = 0.009870\n",
      "Epoch: 0106 loss = 0.010004\n",
      "Epoch: 0107 loss = 0.008692\n",
      "Epoch: 0108 loss = 0.009309\n",
      "Epoch: 0109 loss = 0.007634\n",
      "Epoch: 0110 loss = 0.007573\n",
      "Epoch: 0111 loss = 0.007945\n",
      "Epoch: 0112 loss = 0.006786\n",
      "Epoch: 0113 loss = 0.007868\n",
      "Epoch: 0114 loss = 0.006929\n",
      "Epoch: 0115 loss = 0.006778\n",
      "Epoch: 0116 loss = 0.006574\n",
      "Epoch: 0117 loss = 0.006020\n",
      "Epoch: 0118 loss = 0.005598\n",
      "Epoch: 0119 loss = 0.005717\n",
      "Epoch: 0120 loss = 0.005356\n",
      "Epoch: 0121 loss = 0.005589\n",
      "Epoch: 0122 loss = 0.004947\n",
      "Epoch: 0123 loss = 0.004624\n",
      "Epoch: 0124 loss = 0.005408\n",
      "Epoch: 0125 loss = 0.004710\n",
      "Epoch: 0126 loss = 0.004528\n",
      "Epoch: 0127 loss = 0.004261\n",
      "Epoch: 0128 loss = 0.004457\n",
      "Epoch: 0129 loss = 0.004115\n",
      "Epoch: 0130 loss = 0.004810\n",
      "Epoch: 0131 loss = 0.004263\n",
      "Epoch: 0132 loss = 0.003837\n",
      "Epoch: 0133 loss = 0.003649\n",
      "Epoch: 0134 loss = 0.003518\n",
      "Epoch: 0135 loss = 0.003507\n",
      "Epoch: 0136 loss = 0.003336\n",
      "Epoch: 0137 loss = 0.003292\n",
      "Epoch: 0138 loss = 0.003240\n",
      "Epoch: 0139 loss = 0.003713\n",
      "Epoch: 0140 loss = 0.003017\n",
      "Epoch: 0141 loss = 0.003156\n",
      "Epoch: 0142 loss = 0.002908\n",
      "Epoch: 0143 loss = 0.002753\n",
      "Epoch: 0144 loss = 0.002884\n",
      "Epoch: 0145 loss = 0.002596\n",
      "Epoch: 0146 loss = 0.003135\n",
      "Epoch: 0147 loss = 0.002712\n",
      "Epoch: 0148 loss = 0.002754\n",
      "Epoch: 0149 loss = 0.002566\n",
      "Epoch: 0150 loss = 0.002492\n"
     ]
    }
   ],
   "source": [
    "for ep in range(150):\n",
    "\n",
    "\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for en_input,de_input,de_label in loader:\n",
    "\n",
    "        en_input, de_input, de_label = en_input.to(device),de_input.to(device),de_label.to(device)\n",
    "\n",
    "        #torch.Size([9, 30]) torch.Size([9, 30]) torch.Size([9, 30])\n",
    "\n",
    "        de_pre_y = transformer_model(en_input, de_input) #torch.Size([270, 305])\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=0)(de_pre_y, de_label.view(-1)) #torch.Size([270, 305]) torch.Size([270])\n",
    "\n",
    "        total_loss += loss  # 累加所有句子对的损失\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #break\n",
    "\n",
    "    print('Epoch:', '%04d' % (ep + 1), 'loss =', '{:.6f}'.format(total_loss / len(loader.dataset)))\n",
    "\n",
    "    #break\n",
    "\n",
    "torch.save(transformer_model, 'transformer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 然而 作为地域战略学家 无论是从政治意义还是从经济意义上 让我自然想到的年份是1989年 \n",
      "that naturally comes mind in both politics economics course 1989 politics fall course historical house lehman brothers nothing analogies do europe  or\n"
     ]
    }
   ],
   "source": [
    "transformer_model = torch.load('transformer_model')\n",
    "transformer_model.eval()\n",
    "# 翻译一个英文句子\n",
    "mysentence = mydataset.pairs[3][1]\n",
    "#mysentence = \"but before this new order appears the world may be faced with spreading disorder if not outright chaos\"\n",
    "\n",
    "print(mysentence)\n",
    "en_input = sentence2tensor(chi_lang, mysentence, flag='decoder_in') #torch.Size([30])\n",
    "#--------------------------------------------------------------\n",
    "en_input = en_input.unsqueeze(0).to(device)  # torch.Size([1, 30])\n",
    "\n",
    "start_index = eng_lang.word2index[\"<SOS>\"]  # 2  获取目标语言字典的开始标志位\n",
    "de_input = torch.LongTensor([[]]).to(device)\n",
    "next_index = start_index\n",
    "\n",
    "while True:\n",
    "    # 解码器输入最开始为 标志位SOS 2  逐个预测 拼接 直到结束位EOS 得到最后的dec_input\n",
    "    de_input = torch.cat([de_input.detach(), torch.tensor([[next_index]]).to(device)], -1)\n",
    "    de_pre_y = transformer_model(en_input, de_input) #torch.Size([1, 305])\n",
    "    prob = de_pre_y.max(dim=-1, keepdim=False)[1] #torch.Size([1])\n",
    "\n",
    "    next_index = prob.data[-1] #prob.item()\n",
    "\n",
    "    if next_index == chi_lang.word2index[\"<EOS>\"]:\n",
    "        break\n",
    "\n",
    "word_indexes = de_input.squeeze().cpu()\n",
    "out_words = [eng_lang.index2word[index.item()] for index in word_indexes]\n",
    "out_sentence = ' '.join(out_words[1:])\n",
    "print(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 然而 作为地域战略学家 无论是从政治意义还是从经济意义上 让我自然想到的年份是1989年 \n"
     ]
    }
   ],
   "source": [
    "mysentence = mydataset.pairs[3][1]\n",
    "#mysentence = \"but before this new order appears the world may be faced with spreading disorder if not outright chaos\"\n",
    "\n",
    "print(mysentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
