{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import jieba\n",
    "jieba.setLogLevel(jieba.logging.INFO) #屏蔽jieba分词时出现的提示信息\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "#random.seed(123)\n",
    "np.random.seed(10)\n",
    "\n",
    "#形成由分词后的词汇构成的新数据集，并获得词汇字典（用于词的整数编码）\n",
    "def get_texts_vocab(fn):\n",
    "    max_len = 0\n",
    "    sentence_words = []\n",
    "    vocab_dict = dict()\n",
    "    with open(fn, encoding='UTF-8') as f:\n",
    "        lines = list(f)\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line == '' or '---' in line:\n",
    "                continue\n",
    "            words = list(jieba.cut(line))\n",
    "            words = ['<s>'] + words + ['<e>']\n",
    "            if max_len < len(words):\n",
    "                max_len = len(words)\n",
    "            sentence_words.append(words)\n",
    "            for word in words:\n",
    "                vocab_dict[word] = vocab_dict.get(word, 0) + 1  # 统计词频\n",
    "    f.close()\n",
    "    sorted_vocab_dict = sorted(vocab_dict.items(), key=lambda kv: (kv[1], kv[0]), reverse=True) #按词频降序排列\n",
    "    sorted_vocab_dict = sorted_vocab_dict[:-10]  #减掉10个低频词\n",
    "    vocab_word2index = {'<unk>': 0, '<pad>': 1, '<s>':2, '<e>':3}\n",
    "    for word, _ in sorted_vocab_dict:  # 构建词汇的整数编号，从0，1开始\n",
    "        if not word in vocab_word2index:\n",
    "            vocab_word2index[word] = len(vocab_word2index)\n",
    "    return sentence_words, vocab_word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对给定一个由编码（整数）构成的序列，构建若干个长度为10的序列及其后面的输出词汇编号，即形成等长的输入-输出对\n",
    "#如果给定序列的长度为n，则形成的输入-输出对的数量为n+1\n",
    "def enOneTxt(en_ws):\n",
    "    ln = len(en_ws)\n",
    "    texts,labels = [],[]\n",
    "    for pre_k in range(1,ln):#输入句子的长度为10\n",
    "        ps = pre_k - 10\n",
    "        ps = 0 if ps<0 else ps\n",
    "        pe = pre_k-1\n",
    "        txt = en_ws[ps:pe+1]\n",
    "        txt = txt + [1]*(10-len(txt))\n",
    "        label = en_ws[pre_k]\n",
    "        texts.append(txt)\n",
    "        labels.append(label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_sen_words存放所有的文本行，对其中的每一行进行整数编码（利用字典vocab_word2index），\n",
    "# 然后基于每一个文本行（编码后）生成一系列的输入-输出对，其中输入为长度为10是的整数序列\n",
    "#以所有这样的输入-输出对构成训练数据\n",
    "def enAllTxts(all_sen_words, vocab_w2i):\n",
    "    texts, labels = [], []\n",
    "    for i, words in enumerate(all_sen_words):\n",
    "        en_words = [vocab_w2i.get(word, 0) for word in words]\n",
    "        txts, lbs = enOneTxt(en_words)\n",
    "        texts = texts + txts\n",
    "        labels = labels + lbs\n",
    "    texts, labels = torch.LongTensor(texts), torch.LongTensor(labels)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义自动生成文本的类\n",
    "class Novel_model(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(Novel_model, self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,256)\n",
    "        self.lstm=nn.LSTM(input_size=256,hidden_size=256,batch_first=True,num_layers=1,bidirectional=False)\n",
    "        self.fc1=nn.Linear(256,512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, x):  #torch.Size([128, 10])\n",
    "        o = x\n",
    "        o = self.embedding(o)\n",
    "        o, _ = self.lstm(o) #torch.Size([128, 10, 512])\n",
    "        o = torch.sum(o, dim=1) #torch.Size([128, 512])  torch.Size([128, 1024])\n",
    "\n",
    "        o = self.fc1(o)\n",
    "        o = torch.relu(o)\n",
    "        #o = nn.Dropout(p=0.5)(o)\n",
    "        o = self.fc2(o)  #torch.Size([128, 1524])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524\n"
     ]
    }
   ],
   "source": [
    "#path = r'./data'\n",
    "path = r'C:/Users/86187/data/金庸小说节选.txt'\n",
    "#fn = path+'//'+name\n",
    "sentence_words, vocab_word2index = get_texts_vocab(path)  #构建数据集和编码字典\n",
    "texts, labels = enAllTxts(sentence_words, vocab_word2index) #生成训练数据\n",
    "#torch.Size([3866, 10]) torch.Size([3866]) 1524\n",
    "#print(texts.shape, labels.shape,len(vocab_word2index))\n",
    "vocab_index2word = { index:word for word,index in vocab_word2index.items()} #用于解码\n",
    "vocabsize=len(vocab_word2index)\n",
    "print(vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解决类不平衡问题\n",
    "class_dict = dict()\n",
    "for label in labels:\n",
    "    lb = label.item()\n",
    "    class_dict[lb] = class_dict.get(lb, 0) + 1  # 统计各类别词汇出现的频次\n",
    "weights = []   #跟dataloader.dataset中的数据行要一一对应\n",
    "for label in labels:\n",
    "    lb = label.item()\n",
    "    weights.append(class_dict[lb])\n",
    "weights = 1./torch.FloatTensor(weights)\n",
    "sampler = WeightedRandomSampler(weights=weights, replacement=True,num_samples=len(labels)*1000) #解决类不平衡问题\n",
    "#产生num_samples个下标值，下标取值从0到|weights|-1，个数跟weights中对应的分量值成正比\n",
    "#print(list(sampler)) 每次调用sampler ，结果都不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, sampler=sampler, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_model = Novel_model(vocab_size=vocabsize).to(device)\n",
    "optimizer = torch.optim.Adam(novel_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.36394358\n",
      "0 1.09968126\n",
      "0 0.34231961\n",
      "0 0.34934032\n",
      "0 0.54208618\n",
      "0 0.38508654\n",
      "0 0.52810395\n",
      "0 0.38798892\n",
      "0 0.44423282\n",
      "0 0.73051232\n",
      "0 0.58966219\n",
      "0 0.66991717\n",
      "0 0.50268233\n",
      "0 0.30704257\n",
      "0 0.35659537\n",
      "0 0.20886435\n",
      "0 0.52132004\n",
      "0 0.48871839\n",
      "0 0.53916299\n",
      "0 0.43957132\n",
      "0 0.34751958\n",
      "0 0.71406424\n",
      "0 0.50633329\n",
      "0 0.44096878\n",
      "0 0.49548703\n",
      "0 0.37321284\n",
      "0 0.34253612\n",
      "0 0.19744211\n",
      "0 0.34063211\n",
      "0 0.66461843\n",
      "0 0.57566893\n",
      "0 0.41170195\n",
      "0 1.24675989\n",
      "0 0.39400041\n",
      "0 0.27506232\n",
      "0 0.40869382\n",
      "0 0.44460809\n",
      "0 0.9063167\n",
      "0 0.52035278\n",
      "0 0.78286564\n",
      "0 0.44554409\n",
      "0 0.71382725\n",
      "0 0.38434428\n",
      "0 0.27662665\n",
      "0 0.35494539\n",
      "0 0.94408607\n",
      "0 0.55788499\n",
      "0 0.6178214\n",
      "0 0.34874716\n",
      "0 0.37825084\n",
      "0 0.485589\n",
      "0 0.65273982\n",
      "0 0.74846143\n",
      "0 0.26024702\n",
      "0 0.29873922\n",
      "0 1.10948002\n",
      "0 0.1321712\n",
      "0 0.53866321\n",
      "0 0.20362167\n",
      "0 0.59790659\n",
      "0 0.6688717\n",
      "1 0.4278771\n",
      "1 2.0888567\n",
      "1 0.50510937\n",
      "1 0.24367829\n",
      "1 0.34871712\n",
      "1 0.15950602\n",
      "1 1.01718998\n",
      "1 0.16184306\n",
      "1 0.26244745\n",
      "1 0.20031813\n",
      "1 0.2766017\n",
      "1 0.34952202\n",
      "1 0.58547443\n",
      "1 0.49844155\n",
      "1 0.2421122\n",
      "1 1.42615116\n",
      "1 2.27321839\n",
      "1 0.38969153\n",
      "1 0.30564636\n",
      "1 0.42059112\n",
      "1 1.81904662\n",
      "1 0.54081005\n",
      "1 0.71755517\n",
      "1 0.72624958\n",
      "1 0.16799247\n",
      "1 0.32393876\n",
      "1 0.81849509\n",
      "1 0.66300952\n",
      "1 0.47865656\n",
      "1 0.48204502\n",
      "1 0.24088627\n",
      "1 1.15421796\n",
      "1 0.38601297\n",
      "1 0.47302374\n",
      "1 0.2299969\n",
      "1 0.32454962\n",
      "1 0.5554263\n",
      "1 0.59487683\n",
      "1 0.28062221\n"
     ]
    }
   ],
   "source": [
    "for ep in range(5):\n",
    "    for i, (batch_texts,batch_labels) in enumerate(dataloader):\n",
    "        batch_texts, batch_labels = batch_texts.to(device),batch_labels.to(device)\n",
    "        batch_out = novel_model(batch_texts) #torch.Size([128, 10]) ---> torch.Size([128, 1524])\n",
    "\n",
    "        #print(batch_texts.shape,batch_out.shape)\n",
    "        #exit(0)\n",
    "        #torch.Size([128, 10]) torch.Size([128])\n",
    "        loss = nn.CrossEntropyLoss()(batch_out, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%500==0:\n",
    "            print(ep, round(loss.item(),8))\n",
    "\n",
    "\n",
    "torch.save(novel_model,'novel_model')\n",
    "torch.save(vocab_word2index,'vocab_word2index')\n",
    "torch.save(vocab_index2word,'vocab_index2word') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_model = torch.load('novel_model')\n",
    "vocab_word2index = torch.load('vocab_word2index')\n",
    "vocab_index2word = torch.load('vocab_index2word')\n",
    "novel_model.eval()\n",
    "\n",
    "def getNextWord(s): #给定一个词序列，生成它的下一个词\n",
    "    words = list(jieba.cut(s))\n",
    "    words = ['<s>'] + words #  + ['<e>']\n",
    "    en_words = [vocab_word2index.get(word,0) for word in words]\n",
    "    en_words = en_words[len(en_words)-10:len(en_words)]\n",
    "    en_words = en_words + [1]*(10-len(en_words))\n",
    "    batch_texts = torch.LongTensor(en_words).unsqueeze(0).to(device)\n",
    "    batch_out = novel_model(batch_texts)\n",
    "    batch_out = torch.softmax(batch_out, dim=1)\n",
    "    pre_index = torch.argmax(batch_out, dim=1)\n",
    "    word = vocab_index2word[pre_index.item()]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = '黄蓉'\n",
    "while True:  #生成小说文本\n",
    "    w = getNextWord(seq)\n",
    "    if w=='<e>':\n",
    "        break\n",
    "    seq = seq+w\n",
    "print('生成的小说文本：', seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
